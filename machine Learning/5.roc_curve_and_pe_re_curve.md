## ROC Curve와 Precision-Recall Curve 이해하기

<br>

## 이 글의 목적

이 글에서 배울 Roc Curve와 Precision Recall Curve는 Confusion matrix를 활용한 지표를 시각화하여 표현한 것입니다.

<br>

## 사례로 Confusion Matrix 이해하기

### 배경설명

> 최근 보이스피싱 사기가 급증함에 따라 경찰은 대대적인 단속에 나섰다. 서울 A 경찰청의 보이스피싱 전담팀은 12명의 용의자들을 체포, 조사한 뒤 보이스 피싱범일 가능성이 높은 순으로 이들을 나열하였다.

![untitle](images/ML/data_viz/main_5.png)

A 경찰청 보이스피싱 전담팀은 이중 어디까지를 범인으로 간주해야할지 의견이 엇갈리고 있습니다. 평소 신중에 신중을 가하는 전담팀장은 무고한 피해자를 발생시키지 않기 위해 범인이라 확신이 드는 용의자만 기소해야한다는 의견입니다.

그의 상관인 수사부장은 경찰이 대대적인 단속에 나선만큼 어느정도 의심이 되는 용의자들은 전부 기소해야한다는 의견입니다.

반면 관할경찰청장은 보이스 피싱으로 인한 피해자가 급증하고 있으며 경찰이 대대적에 단속에 나선 만큼, 범인을 최대한 색출하여 더이상 피해자가 발생하지 않도록 철저하게 조사해야 한다는 생각입니다. 그는 무고한 피해자가 발생하더라도 대의를 위해 감내해야 하는 부분이라 생각합니다.
<br><br>

![untitle](images/ML/data_viz/main_6.png)

위 그림은 전담팀장, 수사부장, 경찰청장이 생각하는 용의자 기소 기준을 보여줍니다.

- 신중한 전담팀장은 가장 범인일 가능성이 높은 용의자 3명만 기소해야한다는 의견입니다.
- 수사부장은 확신이 들지 않더라도 범인일 가능성이 높은 2명을 추가해 총 5명의 용의자를 기소해야한다는 의견입니다.
- 경찰청장은 이미 보이스피싱 사기 용의자로 12명을 체포한 만큼 이중 최소 절반을 기소해야한다 입장입니다.

<br><br>

### 기준 선정의 딜레마

![untitle](images/ML/data_viz/main_4.png)

위 그림은 용의자들 중 실제로 누가 범인인지, 시민인지를 보여줍니다. 검은색 아이콘은 실제 보이스피싱 사기범이고 흰색 아이콘은 무고한 시민입니다. 전체 용의자 중 6명은 범인, 6명은 시민임을 알 수 있습니다.

만약 팀장의 의견이 받아들여져서 해당 기준으로 용의자를 기소하게 된다면 무고한 피해자 발생 없이 3명의 범인을 색출할 수 있습니다. 다만 남은 3명의 범인은 기소되지 않아 추가적인 보이스피싱 피해자가 발생할 수 있습니다.
부장의 기준이 받아들여진다면 사기범 6명 중 4명을 기소하지만 그 과정에서 1명의 피해자가 발생하게 됩니다. 마지막으로 경찰청장의 기준이 받아들여진다면 범인 모두를 색출하지만 2명의 피해자가 발생하게 됩니다.

위 예시처럼 누구까지를 범인으로 판단해 기소할지 여부는 절대적이지 않으며 특정 기준을 택해야하는 선택의 문제입니다. 기준은 개인의 신념, 양심, 현재 상황에 따라 다를 수 있습니다. 모든 범인을 잡기 위해서는 무고한 피해자가 발생하는 것을 감내해야하며, 무고한 피해자 발생을 막기 위해서는 일부 범인을 놓아줘야 합니다. 두마리 토끼를 가질 순 없습니다.
<br><br>

> ### Confusion Matrix 이해하기

지금부터 앞선 사례를 통해 Confusion matrix를 설명하도록 하겠습니다.

[confusion Matrix 이미지]

- True Positive : 범인으로 지목된 용의자가 실제로 범인인 경우(예측 성공)
- False Positive : 범인으로 지목된 용의자가 실제로는 범인이 아닌 경우(예측 실패)
- True Negative : 범인으로 지목되지 않은 용의자가 범인이 아닌 경우(예측 성공)
- False Negative : 지목된 용의자가 실제로는 범인인 경우(예측 실패)

  <br>
**Positive**

▸ 전담팀장이 지목한 3명 중 3명이 실제 범인이므로 TP는 3, FP는 1 입니다.

▸ 수사부장이 지목한 5명 중 4명이 실제 범인이므로 TP는 4, FP는 1 입니다.

▸ 경찰청장이 지목한 8명 중 6명이 실제 범인이므로 TP는 6, FP는 2 입니다.

  <br>

**Negative**

▸ 전담팀장이 지목하지 않은 9명 중 6명이 범인이 아니므로 TN은 6, FN은 3 입니다.

▸ 수사부장이 지목하지 않은 7명 중 5명이 범인이 아니므로 TN은 5, FN은 2 입니다.

▸ 경찰청장이 지목하지 않은 4명 중 4명이 범인이 아니므로 TN은 4, FN은 0 입니다.

<br><br>

> Type 1 error와 Type 2 error
>
> 가설검정이론에서 쓰이는 Type 1 error와 Type 2 error는 False Positive와 False Negative의 동의어입니다. 또는 Type 1 error를 False Alarm으로, Type 2 error를 underestimation이라 칭하기도 합니다.

<br><br>

![untitle](images/ML/data_viz/main_7.png)

Confusion Matrix를 읽을 때는 주의해야할 점이 있습니다. 글에 따라 Positive와 Negative 위치가 바뀌는 경우가 잦기 때문에 Label을 주의깊게 봐야합니다. 이 글에서 사용하는 Confusion Matrix는 상단이 Postive 하단이 Negative 입니다.

> ### Precision
>
> Precision은 예측한 값이 얼마나 정확한지를 알려줍니다. Positive로 예상한 instance 중 실제로 얼마나 맞췄는지를 비율로 표현합니다.

  <br>

$\frac{Positive로 \ 예상해 \ 맞춘 \ 경우}{Positive로\ 예측한\ 경우} = \frac{TP}{TP+FP} $

<br><br>

▸ 전담팀장의 Precision은 3명을 범인으로 예측했고(`TP+FP=3`), 3명 모두 범인(`TP=3`)이므로 100%입니다.

▸ 수사부장의 Precision은 5명을 범인으로 예측했고(`TP+FP=5`), 그 중 4명이 범인이므로(`TP=4`) 80%입니다.

▸ 관할청장의 Precision은 8명을 범인으로 예측했고(`TP+FP=8`), 그 중 6명이 범인이므로(`TP=6`) 75%입니다.

<br><br>

> ### Recall
>
> Recall은 실제 Positive 값을 얼마나 찾았는지 알려줍니다. 실제 Positive 중 예측을 통해 얼마만큼 찾았는지를 비율로 표현합니다.

<br>

$\frac{Positive로 \ 예상해 \ 맞춘 \ 경우}{실제 \ Positive인 \ 경우} = \frac{TP}{TP+FN} $

<br><br>

▸ 전담팀장의 Recall은 범인 6명 중(`TP+FN=6`) 실제론 3명만을 찾았기에(`TP=3`) 50%입니다.

▸ 수사부장의 Recall은 범인 6명 중(`TP+FN=6`) 실제론 4명만을 찾았기에(`TP=4`) 67%입니다.

▸ 관할청장의 Recall은 범인 6명 중(`TP+FN=6`) 6명 전부를 찾았으므로(`TP=6`) 100%입니다.

<br><br>

> ### Precision-Recall Trade off

전담팀장, 부장, 청장의 Precision, Recall을 하나의 테이블로 정리했습니다. 테이블을 보면 Precision이 낮아질때 Recall이 올라감을 확인할 수 있습니다.

<br>

|      | Precision | Recall |
| ---: | --------: | -----: |
| 팀장 |      100% |    50% |
| 부장 |       80% |    67% |
| 청장 |       75% |   100% |

<br><br><br>

## Precision-Recall Curve,ROC Curve 해석하기

<br>

> ### 그래프 해석이 어려운 이유

recision-Recall Curve,ROC Curve는 Classification 모델 성능 파악에 매우 유용한 도구입니다. 하지만 이러한 방식으로 그래프를 해석하려고 한다면 앞뒤가 맞지 않는 느낌을 받습니다.

Recall이 증가하니 Precision이 감소하게 되는 건가 싶지만 우리는 이미 Recall과 Precision이 인과관계가 아니라는 것을 알기 때문에 혼란이 발생하게 됩니다.

혼란이 발생하는 이유는 평소 해석하는 그래프들은 x가 변하면 y가 변하는 방식으로 이해되기 때문입니다. $y=ax^2+bx+c$ 와 같이 x값이 증감하면 y값도 증감하는게 일반적으로 우리가 접하는 해석 방식입니다.

Precision-Recall Curve와 ROC Curve를 이해하기 위해서는 기준(Threshold)의 존재를 잊지 않아야 합니다. 보이스피싱범을 기소하기 위한 팀장, 부장, 청장의 기준이 달랐기 때문에 Precision과 Recall이 달라진 것 처럼 Threshold가 변함에 따라 Precision,Recall이 변하게 된다는 사실을 이해 해야합니다.

[Precision-Recall 그래프가 그려지는 과정]

Precision-Recall 그래프가 그려지는 과정을 보면 범인 선정 기준(Threshold)이 한칸씩 이동함에 따라 Precision-Recall이 계산되고 그래프로 그려지고 있음을 확인할 수 있습니다.

Threshold가 맨 왼편에 있다는 말은 모든 용의자를 범인으로 간주하겠다는 의미이고 맨 오른편에 있는 경우 모든 용의자를 범인으로 간주하지 않겠다는 의미입니다. Threshold가 우측에서 좌측으로 한칸씩 이동할수록 범인으로 선정하는 기준이 점차 완화되면서 최종적으로는 모든 사람을 기소하는 경우까지의 Precision과 Recall을 그리게 됩니다.

ROC Curve도 마찬가지입니다. Threshold가 이동함에 따라 TPR과 FPR이 구해지고 이에 따라 그래프가 그려지게 됩니다.

<br>

이처럼 Precision-Recall curve와 ROC Curve가 Threshold의 변화에 따라 그려진다는 사실을 잊지만 않는다면 그래프를 해석하고 이해하는데 어렵지 않습니다.

### ROC Curve 이해하기

<br>

> ### True Positive Rate = Sensitivity = Recall
>
> True Positive Rate는 전체 True 중에서 TP를 얼마만큼 식별했는지 관심이 있다. confusion matrix에서 가장 중요한 항목이 TP이다 보니 True Positive를 활용한 방법에는 여러 이름이 붙게되었다. True Positive Rate를 부르는 다른 중 자주 쓰이는 이름으로는 Recall, Sensitivity이 있다.

용어 이해를 위한 작은 팁을 주자면, 뒤에 Rate가 붙는 용어들 모두 True labels를 분모로 가진다. TPR은 전체 True 중(TP+FN) TP의 비율을 뜻하며 FPR은 전체 False 중(FP+TN) FP의 비율을 뜻한다.

<br><br>

> ### False Positive Rate

FPR은 전체 False(FP+FN)에서 예측이 틀린 경우(FP)가 얼마나 되는지에 관심있다.

<br><br>

> ### 그래프 해석

그래프 시작인 Threshold = 100%인 상태는 어떤 값도 용의자도 범인으로 지목되지 않은 경우이다. Confusion matrix의 Positive 라인이 모두 (0,0)를 확인 할 수 있으며, 따라서 TPR과 FPR 모두 0이된다.

hreshold를 내릴수록 범인이라 판단하는 기준이 낮아지고, 범인으로 지목된 용의자가 많아지면서 TP와 FP 모두 증가한다. FPR과 TPR이 증가하고, 0% 까지 기준을 낮추면 결국 모든 값이 Predict로 이동하여 TPR과 FPR 모두 1에 도달한다.

ROC Curve에서 우리가 관심있게 봐야하는 점은 FPR 대비 TPR의 변화량이다. 모델이 좋을수록 같은 FPR에서 더 높은 TPR을 갖게 된다.

<br><br>

> ### 다른 모델과 비교하기

이번엔 ROC Curve를 활용해 여러 모델을 비교해보자. 이번에는 전국에는 보이스 피싱 전담팀이 A팀 말고도 두 팀이 더 있다. B팀은 경찰청장 직속 팀이라 유능한 형사들로만 팀을 꾸렸고 C팀은 신설된지 얼마되지 않아 경험이 많이 없는 형사들이 주로 있는 팀이다.
<br><br>

A팀 처럼 B팀,C팀도 12명의 용의자를 붙잡고 가장 범인일 것 같은 순으로 나열하였다.
<br><br>

B팀의 예상과 실제 범인 분포이다.
![untitle](images/ML/data_viz/main_8.png)

C팀 예상과 실제 범인 분포이다.
![untitle](images/ML/data_viz/main_9.png)

이렇게 나열한 그림만 본다면 어느 팀이 더 유능한 팀인지 분간하기 어렵다. A~C 세 팀 중 피해자 발생이 적으면서 최대한 많은 범인을 잡아내는 팀은 어디일까?

![untitle](images/ML/data_viz/Roc_compare.png)

A,B,C팀의 ROC Curve를 그려보니 어떤 팀의 성과가 좋은지 바로 확인된다. B팀은 FP Rate 0.2에서 이미 TP Rate가 1에 도달한다. 이에 비해 A팀과 C팀의 TP Rate는 현저히 낮다. 그러므로 B팀이 가장 능력있는 팀이다.

  <!-- <br><br>

  굳이 그래프를 그리지 않고도 모델 성능을 비교할 수 있다.
  누군가에게 설명하거나 지금처럼 연습하지 않는 이상 ROC Curve를 굳이 그릴 필요는 없다. AUC Score을 계산해서 가장 큰 값을 가진 모델이 가장 성능 좋은 모델이다. AUC(Area Under Curve)는 문자 그대로  커브 아래 파란색으로 칠한 면적을 의미한다. AUC가 높아지려면 y값이 빠르게 1에 도달해야 하기 때문에 모델 성능을 보여주는 좋은 지표가 된다. Sklearn에서 AUC를 계산하는 함수가 있으니 이를 사용해 모델 성능을 비교하면 된다.  -->

<br><br><br>

### Precision-Recall Curve

ROC Curve는 실제 True를 얼마나 찾는지와 동시에 실수를 얼마나 적게 하는지를 그래프를 통해 파악한다면, PE/RC Curve는 모델이 얼마나 정확한 예측을 하는지를 그래프를 통해 파악한다.
<br><br>

Precision은 모델의 정확도를 확인하는 좋은 지표이다. 하지만 예측 대비 정확도를 판단하는 Precision의 특성 상 판단 기준을 엄격하게 한다면(threshold를 높힌다면) 필연적으로 Precision을 올라갈 수밖에 없다. Precision이 90%인 모델이 실제로는 1000개 Instance 중 10개만을 Positive로 예측해서 나온 결과라면, 이 모델 좋은 모델인지 판단하기 어렵다.
<br><br>

따라서 Precision을 지표로 활용하기 위해서는 단점을 보완해주는 Recall과 함께 사용해야한다. Recall은 실제 Positive 중 예측으로 실제 Positive를 식별한 경우이다. Recall에는 얼마나 많은 Instance를 예측했는지 설명할 수 있기 때문에 Precision과 함께 사용하면 모델의 예측 정확도를 파악할 수 있다.
<br><br>

PE/RC Curve도 ROC Curve와 마찬가지로 Treshold 변화에 따라 그래프가 그려진다. 기준이 엄격할수록 Precision이 높아지게 되므로 1에서부터 시작하는데, 기준이 낮아질수록 실제 Positive가 포함된 instance와 그렇지 않은 instance가 함께 포함되므로 Precision이 낮아지게 된다. 기준이 낮아질수록 포함하는 instance 수가 증가하고 자연히 Recall도 높아지게 된다. 계속해서 Threshold를 낮춰가다가 0에 도달하게 되면 모든 값을 Positive로 예측하게 되면 Base rate에 도달하게 된다.

{{< test name="pe_rc_curve" dir="/images/ML/plot/pe_rc_curve" >}}

<br><br><br><br>

## 예시 Code

이제는 앞선 설명의 이해를 바탕으로 plot을 그려보자. 제공하는 code는 하나의 참고로서만 활용하고 스스로 그려봤으면 좋겠다. plot을 그려보면서 헷갈리는 개념이 무엇인지, plot을 그리려면 어떤 함수를 써야하는지, 어떤 type의 변수를 넣어야 함수가 작동하는지를 고민하다보면 개념과 실력 모두 내것이 될 수 있다.

<br>

> ### Titanic 데이터 불러오기

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# 데이터 로드 및 분할
a = sns.load_dataset('titanic')
raw_data = a.drop(columns=['alive','who','deck']).dropna()
data = raw_data.drop(columns='survived')
target = raw_data['survived']
```

<br><br>

> ### 인코딩

```python
from sklearn.preprocessing import OrdinalEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

#encoding
ordinal = OrdinalEncoder()
ordinal_col = ['pclass','sex','embarked','class','adult_male','embark_town','alone']
standard = StandardScaler()
standard_col = ['age','sibsp','parch','fare']

preprocessing = ColumnTransformer([
    ('ordinal', ordinal, ordinal_col),
    ('standard', standard,standard_col)
])
```

<br><br>

> ### Logistic Regression으로 모델 학습
>
> plot을 그리는게 주목적이므로 Cross_validaion은 생략했다.

```python
#모델 학습
from sklearn.pipeline import make_pipeline
from sklearn.linear_model import LogisticRegression
model = make_pipeline(preprocessing, LogisticRegression())
model.fit(data,target)

# 모델 성능 및 base_rate 확인
print(f'logistic : {model.score(data,target)}')
print(f'base_rate : {target.mean()}')

# logistic : 0.8216292134831461
# base_rate : 0.4044943820224719
```

<br><br>

> ### Sklearn 모듈에 있는 Confusion Matrix 활용
>
> 알고리즘에 따라 instance를 구분하는 방법이 다르다. logistic regression의 경우 확률을 활용해 대상을 구분한다면 SGDclassifier는 점수를 활용해 대상을 구분한다. 개별 instance별 확률 또는 점수를 알고 싶으면 .predict_proba 또는 .predict_score 함수를 사용하면 된다. 예시에는 Logistic regression을 활용했기 때문에 .predict_probar를 사용했다.

```python
from sklearn.metrics import confusion_matrix
proba_each_row = model.predict_proba(data)
# predict_proba : instance별 예측 값 도출

thereshhold = 0.5
con_max = confusion_matrix(y_true=target,
                           y_pred=(proba_each_row[:,1] > thereshhold),
                           labels=[1, 0])
                           # labels=[1, 0] 넣은 이유 :
                           # 0 = Negative, 1 = Positive를 의미하므로
                           # label을 설정하지 않으면 Positive와 Negative가 바뀐체로 나온다.


# confusion matrix 시각화
sns.heatmap(con_max,
            xticklabels= ['positive','Negative'],
            yticklabels=['True','False'],
            annot=True,
            cbar=False,
            cmap='Blues',
            fmt='g',
            annot_kws={'size':12})
plt.xlabel('Predict label',fontsize=18)
plt.ylabel('True label',fontsize=18)
```

![untitle](images/ML/data_viz/ML1.png)

<br><br>

> ### Sklearn 모듈에 있는 ROC Curve와 Precision_Recall Curve 사용

```python
from sklearn.metrics import roc_curve
from sklearn.metrics import precision_recall_curve

# ROC Curve 자료생성
fpr_logistic,tpr_logistic,thresholds_logistic = roc_curve(y_true=target,
                                                          y_score=proba_each_row[:,1])

# Precision-Recall Curve 자료생성
pr_logistic,rc_logistic, pr_rc_thre_logistic = precision_recall_curve(y_true=target,
                                                                      probas_pred=proba_each_row[:,1])
```

<br><br>

> ### ROC Curve와 Precision-Recall Curve 그리기

```python


plt.style.use('ggplot')
plt.figure(figsize=(14,8))

## ROC Curve 그리기
plt.subplot(121)
plt.plot(fpr_logistic,tpr_logistic,linewidth=2)
plt.title('ROC CURVE',fontsize=18,fontweight="bold",y=1.05)
plt.fill_between(fpr_logistic,tpr_logistic, facecolor='blue',alpha=0.1)
plt.text(0.55,0.4, 'AUC', fontsize=30)

# styling figure
plt.xlabel('False Positive Rate',fontsize=16,labelpad=13,)
plt.ylabel('True Positive Rate',fontsize=16,labelpad=13)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.plot([0,1],[0,1],'k--')
plt.xlim(-0.01,1.01)
plt.ylim(-0.01,1.01)



# Precision-Recall Curve 그리기
plt.subplot(122)
plt.plot(rc_logistic,pr_logistic)
base_rate = target.mean()
plt.plot([0,1],[base_rate,base_rate],'k--')

#styling figure
plt.title('Precision \ Recall Cureve',fontsize=18,fontweight="bold",y=1.05)
plt.xlabel('Recall',fontsize=16,labelpad=13)
plt.ylabel('Precision',fontsize=16,labelpad=13,)
plt.xticks(fontsize=14)
plt.yticks(fontsize=14)
plt.xlim(-0.01,1.01)
plt.ylim(base_rate-0.05,1.01)


plt.tight_layout()
```

![untitle](images/ML/data_viz/main_1.png)

<!-- ## 목차
* Confusion Matrix 이해 - 범인 찾기
* Roc Curve 및 Precision Recall Curve 해석
* 예시 Code

<br><br>

이런 장점에도 불구하고 데이터 분야 외 사람들과 소통할 때 confusion matrix를 알고 있는 사람이 아니고서 confusion matrix를 바탕으로 모델 성능을 설명하기에는 어려움이 따른다.

용어, 헷갈린다. 동의어가 많다.


두 그래프는 confusion matrix에서 모델 성능과 관련된 부분을 그린거라 confusion matrix를 이해하고 있는 사람들에게는 잘 활용돼 하지만 이게 단점이기도 해 직관적으로 이해되기에는 confusion matrix를 잘알아야 하고 용어도 잘 알아야해  근데 우리가 상대해야할 사람은 data를 잘 알고 있지 못하는 사람들이잖아. 그래서 이를 직관적으로 이해할수 있도록 약간 변형을 가한 그래프도 있어. 이 주제는 현재 페이지에 벗어나니까 다음에 다뤄보도록 할게
<br><br>




이 글의 목적은 궁극적인 목적은 단순해. ROC curve와 PE/RC Curve를 해석하는 방법을 배우고 직접 그려보면서 Confusion matrix 개념을 단단히 하는 것이지. 만약 Confuion Matrix와 관련 용어에 대해 익숙하다고 생각하는 사람 있다면 이 글을 읽기 전 2시간 정도 본인이 직접 그려보는 것을 추천해. 우리의 주된 목적은 Confusion matrix를 이해하는 것이지만 그래프를 그리려면 Sklearn과 matplotlib, seaborn과 같은 visualization tool을 잘 다뤄야 하거든. 그래서 직접 구현한 code별로 알았으면 좋겠거나 설명이 필요하다고 생각되는 부분에 있어서는 설명을 하고 넘어가려고 해.
<br><br>

마지막으로 이 글을 단순히 개념을 파악하는데 있다고 생각하지 않았으면 좋겠어. 개념을 이해하는 것 뿐만 아니라 직접 모델도 만들어보고 visualization tool에도 익숙해지는 과정인거야. 그려려면 스스로 그래프를 그려보던 글을 읽으면서 하나씩 따라하며 그려보던 직접 해봐야해. 가장 빠른길이니까 믿고 따라와.
<br><br><br><br> -->

<!-- # 한 통계에 의하면 대한민국 왼손잡이 비율이 5%라고 한다. 대한민국 인구 중 무작위로 100명을 선발한다고 하면 평균 95명은 오른손잡이가 되겠다. 철수는 왼손잡이라서 평소 오른손 잡이 전용으로 만들어진 제품을 사용하면서 큰 불편함을 느꼈다. 이러한 불편한 덕분에 철수는 좋은 사업 아이디어를 떠올리게 됐다. 그는 왼손잡이 용품만 취급하는 쇼핑몰을 운영해 왼손잡이의 마음을 분석하고 사로잡겠다는 전략이었다.

# 머신러닝에 관심많은 그는 사람들의 여러 특성을 종합해 왼손잡이를 예측하는 모델을 우선 개발하기로 결심한다. 최대한 많은 왼손잡이들에게 쇼핑몰을 홍보해 이윤을 극대화해야겠다는 생각 때문이었다. 하지만 그의 실력이 부족했던지 모델 정확도는 85% 이상을 넘지 못했다.

# 마음을 바꿔 kaggle에 거액의 상금을 걸어 왼손잡이 문제를 해결하기로 결심한다. 그가 조건으로 내세웠던 가장 높은 정확도를 달성한 A 모델에게 상금이 돌아갔다. 철수는 94.8%의 정확도를 가진 모델을 가지고 왼손잡이를 찾아 마케팅을 진행하기로 했다.

# 그가 생각하기론 아무리 94.8%라도 현실에는 오차가 크게 발생 할 수 있으므로 마케팅을 수행하는 대상의 약 80%까지가 왼손잡이일거라 예측하고 그중에서 30%는 쇼핑몰에 방문 할 거라 예상했다. 즉 아무리 낮아도 전체 마케팅 대상 중에서 24%정도는 쇼핑몰에 방문한다는 예측이었다.몇주 뒤 마케팅 결과 보고서를 받은 그는 충격을 금치 못했다. 마케팅 대상 중 1%만이 쇼핑몰에 방문했기 때문이었다.

# 모델 정확도 94.8%를 찍은 A모델은 사실 모든 instance를 오른손잡이로 했고 94.8%의 높은 정확도가 나온 이유는 인구의 95%가 오른손잡이였을 뿐이었다.

# ### xxx
# Classification 문제에서 정확도는 모델의 성능을 확인하는 지표가 되기 어렵다. confusion matrix는 모델의 성능을 낫낫히(?) 파헤치는 아주 유용한 도구이다. ~~~~ -->
