{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = 'mps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Gradient Descent Algorithm\n",
    "\n",
    "the goal of learning algorithms = minimizing the risk function\n",
    "> risk = cost = loss\n",
    "\n",
    "gradient descent = powerful optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network function = $y=f_w(x)$\n",
    "> tuning ê°€ëŠ¥í•œ wë¥¼ í¬í•¨í•œ í•¨ìˆ˜\n",
    "\n",
    "A loss function = $L=âˆ‚(y,data)$\n",
    "> nnì˜ ì¶œë ¥ê°’(y)ê³¼ ê²°ê³¼ê°’(data)ì„ ë¹„êµ\n",
    "\n",
    "Optimization problem : $w^* = argmin_wâˆ‚(f_w(x),data)$\n",
    "> ìµœì ì˜ wê°’ì€ loss_functionì˜ ìµœì†Ÿê°’ì¸ ê²½ìš°ë¥¼ ë§í•¨. ìœ„ ë‘ í•¨ìˆ˜ë¥¼ í•©ì¹œ ê²ƒ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### íŠ¹ì • functionì˜ GradientëŠ” í•­ìƒ ê°€ì¥ ê°€íŒŒë¥¸ ìƒìŠ¹ ë°©í–¥ì„ ì§€ëª©í•œë‹¤ê³  í•œë‹¤..\n",
    "\n",
    "**íŠ¹ì • functionì˜ Gradient vector ì°¾ê¸°(ê³µì‹ ìœ ë„)**\n",
    "\n",
    "* ì˜ˆì‹œ\n",
    "$\\begin{equation}\n",
    "z = h(x, y) = \\sin(x^2 + y^2)\n",
    "\\end{equation}$\n",
    "\n",
    "* gradient vector ì°¾ê¸°\n",
    "$\\begin{equation}\n",
    "  \\begin{bmatrix}\n",
    "  \\dfrac{\\partial z}{\\partial x} \\\\ \\\\ \\dfrac{\\partial z}{\\partial y}\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain Rule ì´í•´í•˜ê¸°**\n",
    "\n",
    "* ê¸°ë³¸ ë£°\n",
    "$\\begin{equation}F(x) = g(h(x)) \\equiv (g \\circ h)(x)\\end{equation}$\n",
    "\n",
    "* ë¯¸ë¶„ í•  ê²½ìš° \n",
    "$\\begin{equation}\n",
    "F'(x) = g'(h(x)) \\cdot h'(x)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ê²°ê³¼\n",
    "![as](W1D2_Tutorial1_Solution_115a15ba_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê°„ëµí•œ gradient descentì˜ ì—­ì‚¬**\n",
    "\n",
    "In 1847, Augustin-Louis Cauchy used negative of gradients to develop the Gradient Descent algorithm as an iterative method to minimize a continuous and (ideally) differentiable function of many variables.\n",
    "\n",
    "<br>\n",
    "\n",
    "### ìµœì¢… ê²°ë¡  : gradientëŠ” ìµœê³  ë†’ì€ ìœ„ì¹˜ë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì¸ê±°ê³  ê·¸ gradientë¥¼ negative í•œ ì‹ì´ gradient descentì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent ê·¸ë˜í”„ ì´í•´í•˜ê¸°\n",
    "\n",
    "![a](gradient_1.png)\n",
    "* gradientë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë©´ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ $\\theta_0$ ì™€ $\\theta_1$ë¥¼ ê²©ìë¡œ ìƒì„±í•´ì„œ lossë¥¼ í•˜ë‚˜í•˜ë‚˜ êµ¬í•´ì•¼í•¨.\n",
    "\n",
    "* ì´ëŸ°ì‹ìœ¼ë¡œ ì¼ì¼ì´ êµ¬í•œ ë’¤ ìµœì†Œì˜ loss ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” $\\theta_0$ ì™€ $\\theta_1$ë¥¼ ì°¾ëŠ” ê²ƒì€ ë§¤ìš° ë¹„íš¨ìœ¨ ì ì„\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![a](gradient_2.png)\n",
    "\n",
    "* gradientë¼ëŠ” ê°œë… ìì²´ê°€ ë§¤ ì—°ì‚° ë‹¤ìŒ ë‹¨ê³„ ì¤‘ ìµœì ì˜ ë°©í–¥ì„ ë¯¸ë¦¬ ì•Œë ¤ì¤Œ\n",
    "\n",
    "* ë”°ë¼ì„œ ê²€ì€ìƒ‰ ì„ ê³¼ ê°™ì´, ê°€ì¥ íš¨ìœ¨ì ì¸ ê°’ì„ ì°¾ì•„ê°€ëŠ” ê³¼ì •ì„ ìˆ˜í–‰í•˜ëŠ” gradient descentë¥¼ ì‚¬ìš©í•´ì•¼í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla f \\left( \\mathbf{w}^{(t)} \\right)\n",
    "\\end{equation}$\n",
    "\n",
    "* $\\eta$ = learning Rate\n",
    "\n",
    "\n",
    "$\\nabla f (\\mathbf{w})= \\left( \\frac{\\partial f(\\mathbf{w})}{\\partial w_1}, ..., \\frac{\\partial f(\\mathbf{w})}{\\partial w_d} \\right)$ ì„ ì•Œë©´ ë‹¤ìŒ w ê°’ì„ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤.\n",
    "\n",
    "### ê²°ë¡  \n",
    "Since negative gradients always point locally in the direction of steepest descent, the algorithm makes small steps at each point towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìµœì ì˜ í•´ë¥¼ êµ¬í•˜ëŠ” ê²ƒì„ psudo codeë¡œ í‘œí˜„í•˜ë©´??\n",
    "\n",
    "> Inputs: initial guess ğ°(0), step size ğœ‚>0, number of steps ğ‘‡.\n",
    "\n",
    "> For ğ‘¡=0,1,2,â€¦,ğ‘‡âˆ’1 do\n",
    ">\n",
    "> &nbsp;  **ğ°(ğ‘¡+1)=ğ°(ğ‘¡)âˆ’ğœ‚âˆ‡ğ‘“(ğ°(ğ‘¡))**\n",
    ">\n",
    "> end\n",
    "\n",
    "> Return: ğ°(ğ‘¡+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational Graphë¥¼ í™œìš©í•´  $\\nabla f (\\mathbf{w})$ êµ¬í•´ë³´ê¸°**\n",
    "\n",
    "**ì˜ˆì‹œ í•¨ìˆ˜**\n",
    "\n",
    "$\\begin{equation}\n",
    "f(x, y, z) = \\tanh \\left(\\ln \\left[1 + z \\frac{2x}{sin(y)} \\right] \\right)\n",
    "\\end{equation}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\nabla f (\\mathbf{w})$ ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•**\n",
    "\n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial Loss}{\\partial \\mathbf{w}} = \\left[ \\dfrac{\\partial Loss}{\\partial w_1}, \\dfrac{\\partial Loss}{\\partial w_2} , \\dots, \\dfrac{\\partial Loss}{\\partial w_d} \\right]^{\\top}\n",
    "\\end{equation}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "* ìš°ë¦¬ê°€ êµ¬í•˜ê³ ì í•˜ëŠ” ê°’ë“¤(1~d) : $\\dfrac{\\partial Loss}{\\partial w_d}$\n",
    "\n",
    "* $\\dfrac{\\partial Loss}{\\partial w_d}$ì„ í•˜ë‚˜ì”© êµ¬í•´ë³´ì."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph(forward) - ì›ë˜ ê°œë…\n",
    "\n",
    "* ìœ„ ê³µì‹ì˜ ê³„ì‚° ê³¼ì •ì„ í•˜ë‚˜í•˜ë‚˜ ëœ¯ì–´ë‚´ ë„ì‹í™” í•˜ì˜€ìŒ\n",
    "\n",
    "* x,y,zë¥¼ ëŒ€ì…í•´ $f$ë¥¼ ì¶œë ¥ í•¨\n",
    "\n",
    "    ![a](comput_graph_forward.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Computational Graph(backward) - ë¸íƒ€ ê°’ êµ¬í•˜ê¸° ìœ„í•´ ì‘ìš©\n",
    "\n",
    "* $f$ ê°’ì„ ê°€ì§€ê³  $\\dfrac{\\partial f}{\\partial x}$, $\\dfrac{\\partial f}{\\partial y}$, $\\dfrac{\\partial f}{\\partial z}$ ê°’ì„ ì¶œë ¥í•¨\n",
    "\n",
    "* íŒŒë€ìƒ‰ ë„¤ëª¨ ë°•ìŠ¤ëŠ” ë¯¸ë¶„í•œ ê°’\n",
    "\n",
    "    ![a](comput_graph_backward.png)\n",
    "\n",
    "\n",
    "### ê²°ë¡  \n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial e}~\\dfrac{\\partial e}{\\partial d}~\\dfrac{\\partial d}{\\partial c}~\\dfrac{\\partial c}{\\partial a}~\\dfrac{\\partial a}{\\partial x} = \\left( 1-\\tanh^2(e) \\right) \\cdot \\frac{1}{d+1}\\cdot z \\cdot \\frac{1}{b} \\cdot 2\n",
    "\\end{equation}$ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
