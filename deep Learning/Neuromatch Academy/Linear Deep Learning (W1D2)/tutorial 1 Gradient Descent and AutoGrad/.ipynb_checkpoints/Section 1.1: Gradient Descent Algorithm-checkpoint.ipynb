{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = 'mps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Gradient Descent Algorithm\n",
    "\n",
    "the goal of learning algorithms = minimizing the risk function\n",
    "> risk = cost = loss\n",
    "\n",
    "gradient descent = powerful optimization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network function = $y=f_w(x)$\n",
    "> tuning 가능한 w를 포함한 함수\n",
    "\n",
    "A loss function = $L=∂(y,data)$\n",
    "> nn의 출력값(y)과 결과값(data)을 비교\n",
    "\n",
    "Optimization problem : $w^* = argmin_w∂(f_w(x),data)$\n",
    "> 최적의 w값은 loss_function의 최솟값인 경우를 말함. 위 두 함수를 합친 것\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 특정 function의 Gradient는 항상 가장 가파른 상승 방향을 지목한다고 한다..\n",
    "\n",
    "**특정 function의 Gradient vector 찾기(공식 유도)**\n",
    "\n",
    "* 예시\n",
    "$\\begin{equation}\n",
    "z = h(x, y) = \\sin(x^2 + y^2)\n",
    "\\end{equation}$\n",
    "\n",
    "* gradient vector 찾기\n",
    "$\\begin{equation}\n",
    "  \\begin{bmatrix}\n",
    "  \\dfrac{\\partial z}{\\partial x} \\\\ \\\\ \\dfrac{\\partial z}{\\partial y}\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chain Rule 이해하기**\n",
    "\n",
    "* 기본 룰\n",
    "$\\begin{equation}F(x) = g(h(x)) \\equiv (g \\circ h)(x)\\end{equation}$\n",
    "\n",
    "* 미분 할 경우 \n",
    "$\\begin{equation}\n",
    "F'(x) = g'(h(x)) \\cdot h'(x)\n",
    "\\end{equation}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 결과\n",
    "![as](W1D2_Tutorial1_Solution_115a15ba_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**간략한 gradient descent의 역사**\n",
    "\n",
    "In 1847, Augustin-Louis Cauchy used negative of gradients to develop the Gradient Descent algorithm as an iterative method to minimize a continuous and (ideally) differentiable function of many variables.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 최종 결론 : gradient는 최고 높은 위치를 선택하는 방식인거고 그 gradient를 negative 한 식이 gradient descent이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent 그래프 이해하기\n",
    "\n",
    "![a](gradient_1.png)\n",
    "* gradient를 사용하지 않으면 위 그림과 같이 $\\theta_0$ 와 $\\theta_1$를 격자로 생성해서 loss를 하나하나 구해야함.\n",
    "\n",
    "* 이런식으로 일일이 구한 뒤 최소의 loss 값을 나타내는 $\\theta_0$ 와 $\\theta_1$를 찾는 것은 매우 비효율 적임\n",
    "\n",
    "\n",
    "<br/>\n",
    "\n",
    "![a](gradient_2.png)\n",
    "\n",
    "* gradient라는 개념 자체가 매 연산 다음 단계 중 최적의 방향을 미리 알려줌\n",
    "\n",
    "* 따라서 검은색 선과 같이, 가장 효율적인 값을 찾아가는 과정을 수행하는 gradient descent를 사용해야함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Gradient Descent Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{equation}\n",
    "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla f \\left( \\mathbf{w}^{(t)} \\right)\n",
    "\\end{equation}$\n",
    "\n",
    "* $\\eta$ = learning Rate\n",
    "\n",
    "\n",
    "$\\nabla f (\\mathbf{w})= \\left( \\frac{\\partial f(\\mathbf{w})}{\\partial w_1}, ..., \\frac{\\partial f(\\mathbf{w})}{\\partial w_d} \\right)$ 을 알면 다음 w 값을 계산할 수 있다.\n",
    "\n",
    "### 결론 \n",
    "Since negative gradients always point locally in the direction of steepest descent, the algorithm makes small steps at each point towards the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 최적의 해를 구하는 것을 psudo code로 표현하면??\n",
    "\n",
    "> Inputs: initial guess 𝐰(0), step size 𝜂>0, number of steps 𝑇.\n",
    "\n",
    "> For 𝑡=0,1,2,…,𝑇−1 do\n",
    ">\n",
    "> &nbsp;  **𝐰(𝑡+1)=𝐰(𝑡)−𝜂∇𝑓(𝐰(𝑡))**\n",
    ">\n",
    "> end\n",
    "\n",
    "> Return: 𝐰(𝑡+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Computational Graph를 활용해  $\\nabla f (\\mathbf{w})$ 구해보기**\n",
    "\n",
    "**예시 함수**\n",
    "\n",
    "$\\begin{equation}\n",
    "f(x, y, z) = \\tanh \\left(\\ln \\left[1 + z \\frac{2x}{sin(y)} \\right] \\right)\n",
    "\\end{equation}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\nabla f (\\mathbf{w})$ 를 구하는 방법**\n",
    "\n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial Loss}{\\partial \\mathbf{w}} = \\left[ \\dfrac{\\partial Loss}{\\partial w_1}, \\dfrac{\\partial Loss}{\\partial w_2} , \\dots, \\dfrac{\\partial Loss}{\\partial w_d} \\right]^{\\top}\n",
    "\\end{equation}$\n",
    "\n",
    "<br/>\n",
    "\n",
    "* 우리가 구하고자 하는 값들(1~d) : $\\dfrac{\\partial Loss}{\\partial w_d}$\n",
    "\n",
    "* $\\dfrac{\\partial Loss}{\\partial w_d}$을 하나씩 구해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph(forward) - 원래 개념\n",
    "\n",
    "* 위 공식의 계산 과정을 하나하나 뜯어내 도식화 하였음\n",
    "\n",
    "* x,y,z를 대입해 $f$를 출력 함\n",
    "\n",
    "    ![a](comput_graph_forward.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Computational Graph(backward) - 델타 값 구하기 위해 응용\n",
    "\n",
    "* $f$ 값을 가지고 $\\dfrac{\\partial f}{\\partial x}$, $\\dfrac{\\partial f}{\\partial y}$, $\\dfrac{\\partial f}{\\partial z}$ 값을 출력함\n",
    "\n",
    "* 파란색 네모 박스는 미분한 값\n",
    "\n",
    "    ![a](comput_graph_backward.png)\n",
    "\n",
    "\n",
    "### 결론 \n",
    "\n",
    "$\\begin{equation}\n",
    "\\dfrac{\\partial f}{\\partial x} = \\dfrac{\\partial f}{\\partial e}~\\dfrac{\\partial e}{\\partial d}~\\dfrac{\\partial d}{\\partial c}~\\dfrac{\\partial c}{\\partial a}~\\dfrac{\\partial a}{\\partial x} = \\left( 1-\\tanh^2(e) \\right) \\cdot \\frac{1}{d+1}\\cdot z \\cdot \\frac{1}{b} \\cdot 2\n",
    "\\end{equation}$ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
