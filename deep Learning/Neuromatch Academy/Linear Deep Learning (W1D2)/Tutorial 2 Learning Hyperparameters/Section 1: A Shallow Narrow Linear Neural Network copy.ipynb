{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Function that controls randomness. NumPy and random modules must be imported.\n",
    "\n",
    "  Args:\n",
    "    seed : Integer\n",
    "      A non-negative integer that defines the random state. Default is `None`.\n",
    "    seed_torch : Boolean\n",
    "      If `True` sets the random seed for pytorch tensors, so pytorch module\n",
    "      must be imported. Default is `True`.\n",
    "\n",
    "  Returns:\n",
    "    Nothing.\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = 'mps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non linear로 이해하면 어려우니 linear로 이해해보자"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.1: A Shallow Narrow Linear Net\n",
    "* 딥러닝 모델의 구조를 이해하기 위해서 linear 모델을 활용해 딥네트워크 구조를 만들었음. \n",
    "\n",
    "* 해당 딥러닝 모델의 함수는 $y = xw_1w_2$ 임. \n",
    "\n",
    "* Gradient Descent를 가지고 Loss function을 최소화하는 $w_1$과 $w_2$를 찾아야함\n",
    "![a](layerNarrowLinearNetwork.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent를 수행하기 위해 Computational Graph 그리기\n",
    "\n",
    "* $loss = (y - w_1 \\cdot w_2 \\cdot x)^2$\n",
    "\n",
    "![a](shallow_narrow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed 2021 has been set.\n"
     ]
    }
   ],
   "source": [
    "def gen_samples(n, a, sigma):\n",
    "  \"\"\"\n",
    "  Generates n samples with\n",
    "  `y = z * x + noise(sigma)` linear relation.\n",
    "\n",
    "  Args:\n",
    "    n : int\n",
    "      Number of datapoints within sample\n",
    "    a : float\n",
    "      Offset of x\n",
    "    sigma : float\n",
    "      Standard deviation of distribution\n",
    "\n",
    "  Returns:\n",
    "    x : np.array\n",
    "      if sigma > 0, x = random values\n",
    "      else, x = evenly spaced numbers over a specified interval.\n",
    "    y : np.array\n",
    "      y = z * x + noise(sigma)\n",
    "  \"\"\"\n",
    "  assert n > 0\n",
    "  assert sigma >= 0\n",
    "\n",
    "  if sigma > 0:\n",
    "    x = np.random.rand(n)\n",
    "    noise = np.random.normal(scale=sigma, size=(n))\n",
    "    y = a * x + noise\n",
    "  else:\n",
    "    x = np.linspace(0.0, 1.0, n, endpoint=True)\n",
    "    y = a * x\n",
    "  return x, y\n",
    "\n",
    "\n",
    "\n",
    "class shallowNarrow :\n",
    "\n",
    "    # 임의로 w1과 w2를 지정\n",
    "    def __init__(self, init_weights) -> None:\n",
    "        assert isinstance(init_weights, (list, np.ndarray, tuple))\n",
    "        assert len(init_weights) == 2\n",
    "        self.w1 = init_weights[0]\n",
    "        self.w2 = init_weights[1]\n",
    "\n",
    "    #w1,w2 값에 따른 y값 계산\n",
    "    def forward(self,x) :\n",
    "        y = self.w1*self.w2*x\n",
    "        return y\n",
    "    \n",
    "    # gradient descent를 위한 계산\n",
    "    def dloss_dw(self,x,y_true) :\n",
    "        dloss_dw1 = -(2*self.w2*x*(y_true-self.w1*self.w2*x)).mean()\n",
    "        dloss_dw2 = -(2*self.w1*x*(y_true-self.w1*self.w2*x)).mean()\n",
    "        return dloss_dw1,dloss_dw2\n",
    "\n",
    "    # loss(cost) function 식\n",
    "    def loss(self, y_p, y_t):\n",
    "        mse = ((y_t - y_p)**2).mean()\n",
    "        return mse\n",
    "\n",
    "    \n",
    "    # 훈련시키기\n",
    "    def train(self,x,y_true,lr,n_ep):\n",
    "        loss_records = np.empty(n_ep)  # Pre allocation of loss records\n",
    "        weight_records = np.empty((n_ep, 2))  # Pre allocation of weight records\n",
    "        \n",
    "        for i in range(n_ep):\n",
    "            y_prediction = self.forward(x) # y값 계산\n",
    "            loss_records[i] = self.loss(y_prediction, y_true) # loss function 계산\n",
    "            dloss_dw1, dloss_dw2 = self.dloss_dw(x, y_true) # w1,w2 계산을 위한 값\n",
    "            self.w1 -= lr*dloss_dw1 # 새로운 w1\n",
    "            self.w2 -= lr*dloss_dw2 # 새로운 w2\n",
    "            weight_records[i] = [self.w1, self.w2]\n",
    "        return loss_records,weight_records\n",
    "\n",
    "set_seed(seed=SEED)\n",
    "n_epochs = 211\n",
    "learning_rate = 0.02\n",
    "initial_weights = [1.4, -1.6]\n",
    "x_train, y_train = gen_samples(n=73, a=2.0, sigma=0.2)\n",
    "x_eval = np.linspace(0.0, 1.0, 37, endpoint=True)\n",
    "# Uncomment to run\n",
    "sn_model = shallowNarrow(initial_weights)\n",
    "loss_log, weight_log = sn_model.train(x_train, y_train, learning_rate, n_epochs)\n",
    "y_eval = sn_model.forward(x_eval)\n",
    "# plot_x_y_(x_train, y_train, x_eval, y_eval, loss_log, weight_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2: Gradient Descent 시각화하기\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Blue ribbon**: shows all possible solutions:\n",
    "\n",
    "* **Contour background**: Shows the loss values, red being higher loss\n",
    "\n",
    "* **Vector field (arrows)**: shows the gradient vector field. The larger yellow arrows show larger gradients, which correspond to bigger steps by gradient descent.\n",
    "\n",
    "* **Scatter circles**: the trajectory (evolution) of weights during training for three different initializations, with blue dots marking the start of training and red crosses ( x ) marking the end of training\n",
    "\n",
    "### Saddle Point를 조심하자. Saddle Point는 w이 모두 0인 경우를 가리킨다. 그럼 훈련이 안된다. 그림 정가운데 이동 선이 saddle point이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![a](gradientimg.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
